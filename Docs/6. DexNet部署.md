## 6. DexNet部署

> 想看看DexNet是如何进行部署和仿真等等等等的工作的

### 6.1 整体了解

GQCNN的数据集生成可以通过tools/generate_gqcnn_dataset.py进行生成

数据集是在dropbox中可以直接进行下载的

不过本身由于DexNet需要依赖大量的包,所以有一点不知道该从哪里开始,大概率还是得啃DexNet的代码,然后缺啥弄啥



**备选方案:**

如果实在跑不起来,应该如何操作?比如把里面对应的代码一点点弄出来,然后用这些一系列的python包进行执行,而不采用他的办法;因为DexNet里面其实全部都是拿python文件写的,因此似乎这个思路是可行的



当然感觉这样子的工作量也蛮大的...可是如果代码跑不起来可能...也没办法?





### 6.2 环境搭建

环境搭建采用了Docker进行搭建,本质上dexnet上面的所有代码都是基于python2的,因此硬搞怎么都还算好说



#### 6.2.1 DexNet-PY3

> 最开始是基于Python3进行DexNet的配置的,但是这条路不太对,还是所有都基于python2进行配置彩信g





##### (1) 创建镜像

```
sudo docker run -it -d --name dexnet --gpus all --network host \
    --mount type=bind,source=/home/elevenjiang/Documents/Project/ObjectSynthesis/Code,target=/home/Project/Code \
    -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY -e QT_X11_NO_MITSHM=1 \
    dexnet/setup:v0.1

sudo xhost +local:`docker inspect --format='{{ .Config.Hostname }}' dexnet`
```

前面的内容中,第一行是一系列基本配置,比如-it的交互模式,-d的暂时不进入环境中,--gpus的导入GPU以及最后的网络

第二行的--mount是挂载本地的文件,把当前项目的代码进行了Docker的挂载

第三行是用于进行Docker中可视化的一定的操作



另外,对于新的一行,主要是使能docker内部的可视化操作,否则会导致docker内部无法运行GUI







##### (2) 开始配置

这时候相当于是就重新一步一步地执行install.sh中的内容

###### a. 前期依赖

1. 安装依赖项

   ```
   apt-get update
   
   apt-get install cmake libvtk5-dev python-vtk python-sip python-qt4 libosmesa6-dev meshlab libhdf5-dev
   ```

   

2. 安装python对应代码

   ```
   pip install numpy scipy scikit-learn scikit-image opencv-python pyassimp tensorflow-gpu h5py mayavi==4.5.0 matplotlib catkin_pkg multiprocess==0.70.5 dill cvxopt==1.1.9 ipython pillow pyhull setproctitle trimesh
   ```

   之后在环境配置中,最好还是所有的python包都指定一下版本

   这里面,pip install竟然是直接安装到python3的环境中,但是which pip的时候发现只是针对python



其中的几个python因为没有指定版本,对于python3.6以前的不支持了,只能手动安装:

```
multiprocess:0.70.5
cvxopt:1.2.0

mayavi包安装失败,其中vtk什么的,使用命令:
sudo python3 -m pip install mayavi
mayavi:4.5.0
```





3. 安装SDFGen和Boost.Numpy

   这两个包其实都还算是好装,只是其中sudo需要注意一下,需要apt-get install sudo,否则很多包含sudo的命令无法配置

   其中的Boost.Numpy中发现有很多文件不包含,包含numpy和boost两个库

   ```
   #解决numpy的问题:
   python2中需要安装numpy的库
   
   
   #解决boost的问题:
   apt install libboost-all-dev
   ```



###### b. autolab的包

> 其实都是一堆的python包,只是都要求手动编译

1. core

   这里面要求numpy高于1.16的版本,然后就可以安装了

   安装的时候需要python3 setup.py develop

2. 其他的几个包基本上也就是把python setup.py换成python3 setup.py,然后就问题不大了,剩下的问题就是一些python包太新了装不了

3. perception:

   先安装依赖

   ```
   apt-get install libgeos-dev
   ```

   再提前装一下shapely

   ```
   pip install shapely==1.6.0
   ```

   然后就可以安装了



###### c. dexnet安装

最终就可以进行整体dexnet安装了,基本上没遇到问题

(之前cvxopt装的是1.19的版本,必须要1.2.0版本才行)

不过依然会出现csv_model import CVSModel等的问题



折腾了一堆,发现似乎DexNet是基于2.7而不是python3写的;否则...应该大概率不兼容ROS...因此更换到Python2进行配置

不过其实它内部也有python3的环境,这..就之前的大量工作可能算是作废了;不过基本上排掉了一个问题maybe?





##### (3) Bug解决

发现就算可以安装完成,也有一堆的坑...

###### a. f-string

主要是在autolab-core的文件夹中,可能是都基于3.6之类的高版本写的,从而导致存在语法错误,直接暴力地吧所有f"xxxx" 变成"xxxx" 之后出错再说



###### b. 没有CameraIntrinsic

在perception模块中没有CameraIntrinsics,发现是因为perception的包这几个月更新过,为此,选择旧的branch,即mmatl/semantic_grasping_experiments这个branch进行下载



##### c. 要进行索引的库

**问题:**

```
  File "/root/.local/lib/python2.7/site-packages/skimage/util/arraycrop.py", line 8, in <module>
    from numpy.lib.arraypad import _validate_lengths
ImportError: cannot import name _validate_lengths
```



**解决办法:**

```
apt-get install assimp-utils libassimp-dev
```



##### d. 大量库存在错误

perception的库的问题的版本进行了更改

主要是针对报错

```
Traceback (most recent call last):
  File "test/database_test.py", line 39, in <module>
    from meshpy.obj_file import ObjFile
  File "/home/Project/Code/code/DexNet/DexNet_code/dex-net/deps/meshpy/meshpy/__init__.py", line 4
    print 'Unable to import meshrender shared library! Rendering will not work. Likely due to missing Boost.Numpy'

```



然后是meshpy的版本也要进行更改,尝试换成了dev_varsha的版本,但是这个是python2的版本,明天需要尝试全部基于python2进行环境配置

(当然,发现这个包是2的python包....这个明天肯定要进行python2的测试了)







##### (N) 其他可能帮助

###### a. 指定python库

发现其实dexnet中可以指定







#### 6.2.2 DexNet-PY2

> 一开始不小心全部基于python3进行安装了,但是其实dexnet是基于python2进行配置的(可以简单看它里面的print就知道了)
>
> 因此重新基于python2重新进行一次配置

##### (1) pip2 升级

```
pip install --upgrade "pip < 21.0"
pip2 install --upgrade "pip < 21.0"

#pip版本测试
pip -V
pip2 -V
```



存在 pip._internal的问题

```py
curl https://bootstrap.pypa.io/pip/2.7/get-pip.py -o get-pip.py
python get-pip.py --force-reinstall
```



另外,还有的修复方法是:

```
curl -fsSL https://bootstrap.pypa.io/pip/3.5/get-pip.py | python3.5

#针对的问题:
"/usr/local/lib/python3.5/dist-packages/pip/_internal/cli/main.py", line 60
    sys.stderr.write(f"ERROR: {exc}")
```





##### (2) python环境安装

> DexNet中最头痛的就是很多历史版本的包

```
pip2 install pyassimp tensorflow h5py mayavi matplotlib catkin_pkg multiprocess dill cvxopt ipython pillow pyhull setproctitle trimesh
```



**标准版本**

```
pip2 install --user numpy==1.16 scipy==0.17.0 scikit-learn==0.20 scikit-image==0.14.1 opencv-python==3.4.3.18 pyassimp tensorflow==1.14 tensorflow-gpu==1.14 tensorboard==1.14.0 h5py==2.7  matplotlib==2.2.0 catkin_pkg multiprocess==0.70.5 dill cvxopt==1.2.0 ipython==5.5.0 pillow==4.0.0 pyhull setproctitle trimesh pywavelets==0.4.0 networkx==1.7 mock==2.0.0 apptools==4.5.0 envisage==4.5.1 pyface==5.1.0 kiwisolver==1.0.1 cppy==1.0.2 docutils==0.16 decorator==4.4.2 setuptools==18.5 markdown==2.6.8 pygments==2.5.2 pyyaml==5.4.1
```



##### (3) 依赖包

> 这里面的依赖包必须要基于python2安装的,不能装太新的

autolab_core和visualization都是可以使用pip2安装的

```
pip2 install autolab_core==0.0.7
pip2 install visualization==0.0.6
```



```
SDFGen:master
Boosh.Numpy:master
autolab_core:pre_name_change
perceptoin:dev_danielseita
meshpy:dev_jeff
gqcnn:develop
visualization:master
```

其中,perception的模块需要autolab_core

autolab_core的包使用了pip2 安装

```
pip2 install autolab_core==0.05
```



visualization的包需要一定的依赖项,不过1.0.0是可以基于python2安装的

什么包高了就降一下等级,会有结果的





**又发现了在跑cli中又出了bug,又是有一些包不适配**









##### (4) 可视化bug

目前,grasp和learning两个test下的测试代码是可以成功执行的

但是database不行,一系列可视化的操作依然搞不定,这个就需要重新再来弄了...



**存在问题**

1. meshrender没有

   在database_test.py中,始终是没有meshrender的这个部分

2. 安装pyglet和pyrender

   ```
   pyglet:1.4.10
   pyrender:0.1.45
   ```

   

3. cli中的bug

   ```
   libGL error: No matching fbConfigs or visuals found
   libGL error: failed to load driver: swrast
   Display object failed: Must provide a meshpy.Mesh3D object
   ```

   

4. meiyou bgcolor

   ```
   You can get rid of bgcolor error by changing the line 603 in dexnet/src/dexnet/api.py.
   From:
   vis.figure(bgcolor=(1,1,1), size=(1000,1000))
   To:
   vis.figure(bg_color=(1,1,1), size=(1000,1000))
   ```

   





##### (N) 环境测试

发现或多或少还有一些奇奇怪怪的东西,缺啥装啥,但是最终还是可以吧整体的跑起来,grasping的内容是可以的,不过可视化的部分又不行了...



#### 6.2.3 DexNet-OpenGL

目前感觉是opengl的问题导致了不能使用dexnet,为此直接再在本地进行一次配置,看看能不能跑起来cli的部分;

Docker核心缺少的是:

```
libGL error: No matching fbConfigs or visuals found
libGL error: failed to load driver: swrast
Display object failed: Failed to initialize Pyglet window with an OpenGL 3+ context. If you're logged in via SSH, ensure that you're running your script with vglrun (i.e. VirtualGL). Otherwise, the internal error message was: ""
```

相信这个之后引入了OpenGL的环境应该就能解决这个问题,只是目前暂时不解决这个问题先...





#### 6.2.4 其他bug

> **bgcolor**

目前,抓取的结果展示不了,之前的bug解决了,应该就是opengl的问题

抓取的结果,之前的bgcolor的问题,目前发现了这个办法似乎有解

不过最后还是放弃



> mlab

这个是mayavi中的mlab更不知道为什么没有

整体操作

```
export ETS_TOOLKIT=qt4
sudo apt install python-pyside
```



不过,抓取依然没有内容,因此继续更改:

参考:https://github.com/BerkeleyAutomation/dex-net/issues/26





#### 6.2.5 配置整理

> 最佳的配置应该是Ubuntu16.04+OpenGL底层配置;不过这里面依然是存在问题的,最终grasp的那些点看不到,这个也不清楚是为什么

**配置过程**

​	首先,dexnet配置最恶心的就是他的很多版本都对不上,有一些包的东西没有,而另外的又有,这个要不断地去测试到底哪个有哪个没有是最痛苦的.

​	包分为两批,一个是python本身的包,这个的更换本质上就是一堆的内容进行更换,另一部分是autolab的包,这里面的版本就更乱了,都不知道了具体哪一个是正确的版本.不过其实主要是autolab_core和visualization;perception,meshpy,gqcnn这几个就还好,基本上没有太大的问题

​	最后,需要这些文件都安装到python2的版本上,一定不能安装到python3的版本,否则又会是有一堆的坑

​	基本上,到这里就可以完成对应grasp等的API调用了,但是想要可视化,还需要mayavi这个东西进行,这个可视化的工具也非常糟糕,其实如果真的不行,可以考虑采用GraspNet中的API进行可视化,那里面的open3D肯定会好看很多

​	为此,最后在Ubuntu本地上面再进行一次配置,看看mayavi的显示问题是不是因为部署的不同导致出问题的,如果不是的话那就都还是基于docker进行配置



**最终环境选择:**

基于CUDA GL的环境进行配置

```
docker pull nvidia/cudagl:10.0-devel-ubuntu16.04
```



##### (1) 创建虚拟镜像

> 基于nvidiagl-ubuntu16.04进行配置







##### (2) python2基本库安装

>  基本上没有大问题,遇到哪个版本不对就...降级就好了

**发现其实pip2 安装和pip安装都是针对python2的...**

- 一些搅局的python,需要提前且按顺序进行安装

```
pyface:6.0.0
traitsui:6.0.0
```

- 各种包记录

```
dexnet依赖包:
pip2 install --user numpy==1.16.1 scipy==0.17.0 scikit-learn==0.20 scikit-image==0.14.1 opencv-python==3.4.3.18 pyassimp tensorflow==1.14 tensorflow-gpu==1.14 tensorboard==1.14.0 h5py==2.7  matplotlib==2.2.0 catkin_pkg multiprocess==0.70.5 dill cvxopt==1.2.0 ipython==5.5.0 pillow==4.0.0 pyhull setproctitle trimesh pywavelets==0.4.0 networkx==1.7 mock==2.0.0 apptools==4.5.0 envisage==4.5.1 pyface==5.1.0 kiwisolver==1.0.1 cppy==1.0.2 docutils==0.16 decorator==4.4.2 setuptools==18.5 markdown==2.6.8 pygments==2.5.2 pyyaml==5.4.1 six==1.11.0 
```





##### (3) autolab库

###### a. 整体安装

```
SDFGen:master
Boosh.Numpy:master
autolab_core:pip安装0.05
perceptoin:dev_danielseita
meshpy:master
gqcnn:develop
visualization:pip安装0.0.7
```



###### B. 存在bug

###### b1. boostnumpy

会缺少一些依赖项,需要进行boost等安装

```
sudo apt-get install cmake libblkid-dev e2fslibs-dev libboost-all-dev libaudit-dev
```



###### b2. perception

这里面很依赖一定的python库

```
pip install ffmpeg-python==0.1.5
pip install pyserial keras scikit-video
pip install networkx==1.8
pip install pillow==4.3.0
```

###### b3. meshpy

meshrender安装失败问题:

1. 缺少了glut的包

   这些安装需要在手机热点下,电脑wifi可能有问题

   ```
   sudo apt-get install libgl1-mesa-dev
   sudo apt-get install libglu1-mesa-dev
   sudo apt-get install freeglut3-dev
   ```

2. ```
   NameError: global name 'meshrender' is not defined
    c, d = meshrender.render_mesh([P],
   ```

   

3. 





###### b4. gqcnn

要求tensorboard大于1.14.0,但是tensorboard有要求setuptools大于42,不过死活会有

```
#输入
pip install --upgrade setuptools==42.0.0

#输出:
Not uninstalling setuptools at /usr/lib/python2.7/dist-packages, outside environment /usr
```

导致无法安装,为此需要

```
pip install tensorboard==1.14.0 setuptools==42.0.0
```



**gqcnn单独使用:**

其实gqcnn依然还有在进行维护,只是不再基于python2进行维护,为此gqcnn的安装没有使用master中的路径,而是选择了master的路径;

另外,gqcnn的内容





###### b5. visualization

先装依赖,然后才可以进行安装

```
imageio:2.2.0
meshrender:0.0.1
visualization==0.0.6
```





##### (4) dexnet测试

最终,在dex-net的路径下进行安装就行

测试程序:

**下载文件的路径:**

```
/home/Project/Code/code/DexNet/dex-net/data/test/database/example.hdf5
```

目前,grasp的东西可以显示出来的,只是有抓取点的位置,也有物体模型,但是他们两个不在一个页面上,这个是最神奇的,不过看到github上面有的解



###### a. 存在bug

- validate_lengths

  升级 scikit-image

  ```
  #错误:
  ImportError: cannot import name _validate_lengths
  
  #解决:
  pip install --upgrade scikit-image
  ```

- vis is not defined

  ```
  Display object failed: global name 'vis' is not defined
  ```

  这个是dexnet/api.py中的vis导入出错

  ```
  from dexnet.visualization import DexNetVisualizer3D as vis
  ```

- ScenViewer缺少

  ```
  AttributeError: 'SceneViewer' object has no attribute 'save_directory'
  ```

- 抓取上没有结果

  参考资料:https://github.com/BerkeleyAutomation/dex-net/issues/26

  不够这个可能存在问题

  






##### (5) 其他bug解决

###### a. openrave

需要安装这个,否则碰撞检测相关的东西就没有

参考安装链接:https://zhuanlan.zhihu.com/p/170490647

发现这里面可以随缘安装,部分安装了就能用

```
#1: 安装依赖
sudo apt-get install libassimp-dev libavcodec-dev libavformat-dev libavformat-dev libboost-all-dev libboost-date-time-dev libbullet-dev libfaac-dev libglew-dev libgsm1-dev liblapack-dev liblog4cxx-dev libmpfr-dev libode-dev libogg-dev libpcrecpp0v5 libpcre3-dev libqhull-dev libqt4-dev libsoqt-dev-common libsoqt4-dev libswscale-dev libswscale-dev libvorbis-dev libx264-dev libxml2-dev libxvidcore-dev

#2: 下载openrave安装脚本
git clone https://github.com/crigroup/openrave-installation

#3: 进行安装
#这里面中间两个没有安装,直接第一个和最后一个了,也可以编译安装成功,虽然会出现核心已转储,但是似乎也没有大问题...
sudo ./install-dependencies.sh
sudo ./install-osg.sh
sudo ./install-fcl.sh
sudo ./install-openrave.sh

#4: 测试:
openrave.py --example graspplanning
```





###### b. meshrender

这个看起来是必须要解决了,不解决就导致了很多地方过不去

后面变成了不能在python文件中import meshpy,但是命令行中import是可以的

```
ImportError: libboost_numpy.so: cannot open shared object file: No such file or directory
```

为此,找到libboost_numpy.so,然后粘贴到/usr/lib中







#### 6.2.6 原始配置

> 希望可以不用docker就进行数据增强,因此需要进行对应一系列文件的移植
>
> 但是发现这个东西太复杂了,似乎并不好移植

缺少meshlab的指令,导致meshlabserver用不了

```
sudo apt-get install meshlab
```



存在一堆python2和python3的问题,比如:

```
TypeError: '<=' not supported between instances of 'map' and 'map'
```







### 6.3 代码框架

> DexNet中包含了非常多的代码,其实或多或少是可以的,目前已经可以跑通代码了,接下来就是进行对应的配置等工作

#### 6.3.1 dexnet整体代码

dexnet对应的API文档是:https://berkeleyautomation.github.io/dex-net/tutorials/tutorial.html

##### (1) apps

这里面是一个dexnet_cli.py,这个主要进行了核心功能的展示,这个呈现效果还是不错的

**对应的执行教程:**

https://docs.google.com/document/d/1a9aoDuo-iYG-UyCJPq-ubnyW2Hgnf0YIdYvkp5DlBf0/edit

**database的路径:**

```
/home/Project/Code/code/DexNet/dex-net/data/test/database/example.hdf5
```



###### a. dexnet_cli.py

- 这里面就是开创了一个命令窗口,通过数字选择之后,把对应的命令和执行函数嫁接起来
- 整体是基于dexnet_api的,这个是dexnet.DexNet()的大类进行执行的,这个大类被定义在了dexnet/src/dexnet/api.py中
- 对于数据展示,是通过dexnet/database.py/Hdf5Database的类读取了h5py中的内容,然后进行解析,获得了一系列的数据内容的
- 另外,这里面需要使用到vhacd,这个和blender等的相关,之后有时间了也要学习一下



###### b. open_database

> 这个是用于解析数据集的工作

主要是调用了api.py中的open_database的函数,这个函数又是依赖于database.py重点Hdf5Database的数据集,这个数据集基于h5py的文件打开一个文件进行解析工作等





###### c. display_grasp

发现这里面走到底,确实是没有一起可视化的;这里面用了mlab和visualizer两个可视化的工具进行展示

之后想要在一个图上面进行展示可以基于Visualization的库上面画点解决,或者在mlab中画mesh进行解决



##### (2) cfg

这里面是一系列的参数配置文件,包含了生成数据集,可视化数据集等的yaml文件

这个方法在以后代码量上来之后可以进行参考,就是一个类中的所有变量写在一个yaml中,这样子方便调参,也可以更好地修改参数和记录参数



##### (3) data

这里面存放了gripper的尺寸,几个基本的形状以及一个test.hdf5的测试文件,这个测试文件针对的是bar_clamp这个物体的一些参数展示

解析h5py的代码就在database中



##### (4) deps

这里面就是一堆的autolab的依赖文件了,可以非常解耦地去看



##### (5) examples

这里面是执行抓取配准相关的内容;其实就是展现了如何进行抓取的,没什么太特别的东西



##### (6) src/dexnet

> 这里面是dexnet所有代码的核心所在地了,包括了database,grasping,
>
> learning,visualization;然后在外层又进行了api.py的封装

###### a. api封装



###### b. database

这里面是进行了数据集的解析工作,主要的函数是database.py,

- hdf5_factory.py

  这里面相当于是对h5py文件所有的操作,比如添加数据,写入数据等等的方法

- keys.py

  这里面存储量一个数据集对应的参数表进行说明

- mesh_processer.py

  有一些比如转换成sdf文件的东西





###### C. grasping

> 这个是最期待的文件,里面有大量的grasping相关的内容可以参考,比如碰撞检测,抓质量评估,抓取点采样等

grasping文件夹中主要实现的功能:

1. 碰撞检测

2. 抓取质量评估

3. 抓取点采样

4. 抓取抽象(感觉可能是把爪子和抓取物体都进行了抽象?)

   graspable_object.py

5. 



###### c1. test/grasping_test.py

这里面进行了grasping一系列基本操作的样例测试代码,从constants中导入了需要配置的参数,进行的函数执行测试:

```
GraspTest('test_init_grasp')
GraspTest('test_init_graspable')
GraspTest('test_init_gripper')
GraspTest('test_force_closure')
GraspTest('test_wrench_in_positive_span')
GraspTest('test_min_norm_vector_in_facet')
GraspTest('test_antipodal_grasp_sampler')
GraspTest('test_grasp_quality_functions')
GraspTest('test_contacts')
GraspTest('test_find_contacts')
```

具体的实现代码就在src/grasping的文件夹中进行的





###### d. learning

> 这里面是一些选择的方法,计算方法等,真正的gqcnn还是在deps/gqcnn中实现





###### e. visualization

> 这个和mayavi是两个不同的可视化工具,了解一个就够了,两个都了解或多或少有一些浪费/没有意义





###### f. tools

> 这里面有进行gqcnn数据集生成的东西,还有gqcnn数据集可视化的内容







##### (7) 数据集解析

> dexnet中包含了非常多的数据集,比如物体模型,仿真场景的,gqcnn进行识别的等等,这些都需要进行解析,知道他们对应的数据格式才行

gqcnn的

dex-net跳转的路径是直接到datasets中



**几个数据集路径**

1. dex-net

   https://berkeley.app.box.com/s/jnbwcj3zppcdhg5ffgf47ugqngosm40g/folder/28012674783

   这里面只有dexnet_1.0和dexnet_2.0_training_database的数据,另外还有一个example.hdf5,一个比较久远的数据集

2. gqcnn

   https://berkeley.app.box.com/s/p85ov4dx7vbq6y1l02gzrnsexg6yyayb/folder/24201645969

   这里面包含了datasets,model_zoo,cameras三个大文件夹

   datasets中就是dex-net1,2,3,4中所有包含的数据集,甚至还有fcgqcnn的数据

   另外,datasets中还有adv_synth,aa之类的数据集命名,这个也要去搜索一下

   model_zoo中包含的是gqcnn多个训练的参数保存内容

   这里面的dexnet_2和dexnet_2_detection

   dexnet_2中对应的是仿真的桌子,抓取评估的数据集内容

   dexnet_2_detection差不太多,但是没有使用zip包进行封装,不过里面到底是什么数据也不清楚了,发现读取不了,这个dexnet_2_detection的文件特别特别旧了(2017年的内容),因此不使用



###### a. example.hdf5

1. 对应数据集:

   这个是dexnet数据集仿真生成的数据,有一个13GB的数据内容,还是非常大的

   https://berkeley.app.box.com/s/jnbwcj3zppcdhg5ffgf47ugqngosm40g/folder/28012674783

2. 创建了Database后,会进行load_database和load_datasets两个的操作



对于一个h5py的文件,可以递归地把内部的所有参数进行输出查看

```python
def print_attrs(name, obj): print("************************"+name+"************************")
    for key, val in obj.attrs.items():
        print("    %s: %s" % (key, val))

def see_h5py(dataset_path):
    f=h5py.File(dataset_path,'r')
    f.visititems(print_attrs)
    
if __name__ == '__main__':
    see_h5py("/home/Project/Code/code/DexNet/dex-net/data/test/database/example.hdf5")
```







这个是用于可视化dexnet_cli.py的数据集,里面可以看到物体不同的抓取点等的内容

这个整体我猜测是一个实际的场景,针对仿真环境进行建模,然后把仿真环境中的数据存储起来的数据,包含了抓取位姿等的内容

对于一个database,即一个hdf5的文件

这里面首先基于Hdf5Database进行数据解析,这里面的datasets_是一个list,list中存储了一系列Hdf5Dataset的内容,这个Hdf5Dataset就是内部所解析的所有内容了













###### b. gqcnn

1. 对应数据集

   gqcnn的数据集针对的是文件夹 https://berkeley.app.box.com/s/6mnb2bzi5zfa7qpwyn7uq5atb7vbztng

   中的dexnet_2中的内容;但是其内部的dexnet_2_detection中的内容其实暂时解析不了.

2. 数据集内容包含

   每个文件中包含了13个对应的文件,一共有6729组文件,每组13个,同时还有一个config.json和一个grasp_chache.pkl两个暂存文件

   ```
   hand_poses_06727.npz:(1000, 7)
   robust_ferrari_canny_06727.npz:(1000,)
   object_labels_06727.npz:(1000,)
   binary_ims_raw_06727.npz:(1000, 32, 32, 1)
   depth_ims_tf_table_06727.npz:(1000, 32, 32, 1)
   image_labels_06727.npz:(1000,)
   force_closure_06727.npz:(1000,)
   depth_ims_tf_06727.npz:(1000, 32, 32, 1)
   depth_ims_raw_06727.npz:(1000, 32, 32, 1)
   depth_ims_raw_table_06727.npz:(1000, 32, 32, 1)
   binary_ims_tf_06727.npz:(1000, 32, 32, 1)
   pose_labels_06727.npz:(1000,)
   table_mask_06727.npz:(1000, 32, 32, 1)
   ```

   - depth_ims_tf_table:

     1000x32x32x1:1000张深度图,每张图是32x32的大小

   - hand_poses

     1000x7,描述了机械夹爪的信息,7个数据对应了

     ```
     (row,column)->抓取点
     depth:爪子中心深度,相机中的数据(m制度)
     angle:radians,关于image的x-axis
     
     #下面三个看不太懂
     row_index:这两个可能是目前抓取图片世界坐标系大图片的正中心?
     column index
     width
     ```

   - robust_ferri_canny,feriri_canny,force_closure

     三个都是1000的维度,进行抓取与否的0/1判断

     1000,基于0.002的数值,生成0或1的成功/失败抓取

   

   ```
   hand_poses#1000x7,
   table_mask
   
   pose_labels
   image_labels
   object_labels
   
   depth_ims_tf
   binary_ims_tf
   depth_ims_tf_table:1000x32x32x1,对应的桌面深度图
   
   
   depth_ims_raw
   binary_ims_raw
   
   
   #力闭合
   force_closure
   robust_ferrari_canny
   ```

   

```
hand_poses_06727.npz:(1000, 7)

object_labels_06727.npz:(1000,)
image_labels_06727.npz:(1000,)

binary_ims_raw_06727.npz:(1000, 32, 32, 1)
就是0和255二值化的图,把物体的颜色变成了255,其他部分变成了0;不过也是float64的存储格式
binary_ims_tf_06727.npz:(1000, 32, 32, 1)

table_mask_06727.npz:(1000, 32, 32, 1)
就是物体是黑的,桌子是白的

depth_ims_tf_table_06727.npz:(1000, 32, 32, 1)
深度图,float64,m制0.5~0.7

depth_ims_raw_table_06727.npz:(1000, 32, 32, 1)


robust_ferrari_canny_06727.npz:(1000,)
force_closure_06727.npz:(1000,)

depth_ims_tf_06727.npz:(1000, 32, 32, 1)
这个最小值变成了0,float64,m制0.0~0.74
depth_ims_raw_06727.npz:(1000, 32, 32, 1)
和上面的差距不大的max值,猜测可能有一个是加入了噪声



pose_labels_06727.npz:(1000,)

```











**所有数据集说明**



1. down_example_data.sh

   https://github.com/BerkeleyAutomation/gqcnn/blob/master/scripts/downloads/download_example_data.sh

   这个是直接在gqcnn中找到的,下载样例数据集的方法

   不过这个使用tools中的visualization_gqcnn_dataset.py无法打开

2. dexnet2

   这个位于https://berkeley.app.box.com/s/p85ov4dx7vbq6y1l02gzrnsexg6yyayb/folder/35513569277

   是gqcnn/datasets/dexnet_2_detection的中的文件

   最终针对dexnet_2中的文件,可以用tools/visualize_gqcnn_dataset.py进行解析查看,说白了都是一堆的32*32的图,然后用npy存着,相当于是变换成了一系列的矩阵







###### c. model_meshes

这里面就是一系列的物体模型,对应的路径是:https://berkeley.app.box.com/s/jnbwcj3zppcdhg5ffgf47ugqngosm40g/folder/28012674783

这个是dexnet的存放路径,里面的grasp_databases就是之前文件夹中的datasets

这里面存放了三种物体模型,不过整体的尺寸似乎没有那么的归一化,可能就是针对了某几个方向上进行归一化,但是整体的并没有进行归一化,比如其中的螺丝刀这种长条形的,非常大;这个的处理之后可能也要注意了



本身3d-net的官网:https://strands.readthedocs.io/en/latest/datasets/three_d_net.html

这个太久了,也基本上没有进行维护等的工作...













### 6.4 数据集制作

这是最关键的,本身DexNet2.0有1500个3D Model,我需要对这些3D Model进行扩充等的工作,然后再进行抓取数据集的生成,从而让DexNet进行结果测试等

#### 6.4.1 数据集制作流程

本质上,数据集的制作是DexNet1.0中的内容,然后使用dexnet_cli中的内容生成抓取姿态和物体mesh;然后基于此进行DexNet2.0中的图像对应的gqcnn的生成,进行分类等的任务



##### (1) 生成h5py文件

这里主要就参考database_test.py的函数进行执行,其实本身看不太懂其中的内容,不过这里面包含了几个内容,这个是确定的

###### a. 已有功能

**参考database_test.py**

在database_test.py中,包含了添加物体obj数据,测试了读取写入mesh,测试了从dataset中获取物体的stable_poses等的功能

- 创建和删除database,dataset
- 创建了graspble objec





**参考database.py中的DataBase和Dataset类**

这里面的DataBase还好,基本上就是一个list存了一系列的Hdf5Dataset的类,核心还是Hdf5Dataset的类



###### b. Hdf5Dataset类解析

> 整体中主要是通过store_xxx和create_xxx两种方法进行内容存储

基于```def create```可以也有一种数据类型存储,包含3个数据

```
create_graspable
:即一个graspable object,之后用obj.key进行索引
create_metric
:即一个抓取计算方法的metric,之后用metric进行索引
create_metadata
:即一个metadata类型
```



基于```def store_```就可以看到可以针对Dataset的类存储什么东西进去,包含8个数据

```
store_mesh:
一个obj.key存一个mesh

store_conves_pieces:
一个obj.key存一个list,这个list都是conves_pieces

store_stable_poses:
一个obj.key存一个list,list中都是stable_pose

store_grasps:
一个obj.key存一个list,是针对这个物体生成的所有grasps,此处暂时与stable_pose无关,它内部的grasp_pose_id在这里没用

store_grasp_metrics:
一个obj存一个list,一个grasp一个metric,对应计算的结果


####下面的没那么相关,暂时没细看###
store_rendered_images
:保存图片

store_connected_components
:保存连同组件

store_object_metadata
:保存物体的metadata
```









1. 



##### (2) 生成gqcnn数据

本身包含了初始化过程,制作候选抓取姿态和基于grasp生成图片三个步骤

本质上,需要hdf5提供物体obj,物体stable_pose,物体所有grasps,物体所有grasp_metrics;



###### a. 初始化过程

1. 设置生成数据集路径和config路径

   基于config路径,里面读取本身的database(即hdf5),另外读取三个配置参数:目标生成物体,仿真环境数据(针对摄像头的参数),爪子名称

   最终,一起送入generate_gqcnn_dataset的函数,进行数据生成

   ```
   generate_gqcnn_dataset(dataset_path,database,target_object_keys,env_rv_params,gripper_name,config)
   ```

   

2. 读取一系列生成参数

   - 生成的参数:这里面包含每个stable_pose生成多少个images,以及生成稳定姿态的最小概率p

   - gqcnn的参数,包含crop的长宽和最终图片的长宽

   - 桌子对其参数(这个不是很清楚是用来做什么的,有可能是相机和物体并不是平行的?)

   - 碰撞参数,包含接近碰撞,delta_approach,table_offset三个参数

     同时加入了桌子的obj文件

   - 设置tensor的大小(基于gqcnn数据集的参数)



3. 添加了可获取的metrics,假设所有物体保存的metrics是相同的

   这里面有一系列跟grasp相关的内容,之后需要去查看一下

   

4. 添加了数据集生成的log和数据集生成的config所对应的json文件

   即分别一个log文件一个config文件



###### b. 生成candidate grasp dict

1. 对candidate_grasp_dict中,里面每个obj.key在这个dict,也对应一个dict

2. 初始化碰撞检测算法
3. 对于每个obj.key,获取所有stable_poses,对每个pose进行
   - 引入table的碰撞检测
   - 读取物体的所有grasps
   - 获取相当于桌面的grasps,得到aligned_grasp
   - 然后是每个间隔高度一个grasp,直到collision free为止
4. 把所有的grasp保存成一个candidate_grasps_dict



###### d. 基于grasp生成images

1. 首选保存object_category_map和pose_category_map

   分别对应```obj.key```和```obj.key-stable_pose_id```

2. 从stable_pose_id中拿到候选的candidata_grasp_info

   这个graps_info中包含了grasps和collision_free两个参数

3. 基于grasps拿到对应计算的grasp_metrics

4. 基于均匀采样,一个UniforPlanarWorksuirfaceImageRandomVariable的类,从中进行render_samples的查看

5. 同时每个图获取对应的grasp-metrics等的参数

最终生成图片





##### (3) 总结

###### a. gqcnn需要

对于本身生成的数据,在gqcnn中需要的数据为:

都是对于dataset.xxx而言,每个dataset.xxx均是索引了obj.key,即obj的名称进行获取的

1. stable_pose

2. grasps (这是一个ParallelJawPtGrasp3D的类)

3. grasp_metrics

   这个是到了最后才进行存储的



###### b. database生成参考代码

因此,针对一个hdf5地方文件,说白了,里面需要存储

1. obj本身(同时obj还包含了sdf等)

   这个在test_database.py的代码中有样例,通过MeshProcesser,可以同时生成sdf,mesh等文件,同时还可以生成stable_pose;通过MeshProcessor抽象成了一个graspable的数据

2. stable_poses

   MeshProcessor可以生成,内部是桌子和摄像头之间的变换关系

3. grasps

   grasps的生成在grasping_test.py中的```test_antipodal_grasp_sampler()```的函数中进行了展示,在database_test.py中展示了如何进行grasps的添加;

   

4. grasp_metrics

   这个的创建包含了config和参数两种,config是力闭合的定义等,在database中直接定义为了metrics,而参数就是基于config参数计算出来的力闭合与否的参数,定义为grasp_metrics

   grasp_metrics的生成在grasping_test.py中的```test_grasp_qulity_functions()```有样例代码

   这个的添加方法在database_test中有样例,









#### 6.4.2 生成h5py文件















## 7. GQCNN训练测试

> 接下来进行GQCNN的训练测试工作,从而证明数据增强是有助于整体提升的抓取效果的

### 7.1 gqcnn-py3了解

> 发现dexnet中的很大一部分东西确实非常难配置,基本上摸到了80%的内容,就是数据生成中的渲染部分不是很想继续使用dexnet的内容,另外全部还是基于python3进行开发,所以基于新的代码进行书写

#### 7.1.1 环境配置

> 在Ubuntu160.4 cudagl上进行配置,基于python3.5

##### (1) autolab_core

因为python3.5不支持f-string的模式,因此需要安装pip install future-fstrings 

同时在有这种的python文件中加入一句

```
# -*- coding: future_fstrings -*-
```



##### (2) 对于gqcnn

这里面有一堆的基于autolab_perception import xxxx的东西,都需要改成

```
#原始
from perception import BinaryImage
#更改为
from autolab_core import
```



##### (3) 下载训练模型

在models中下载需要的文件,不同测试

```
#多个物体也可以使用dexnet4.0的参数
python3 examples/policy.py GQCNN-4.0-PJ --depth_image data/examples/clutter/phoxi/dex-net_4.0/depth_0.npy --segmask data/examples/clutter/phoxi/dex-net_4.0/segmask_0.png --camera_intr data/calib/phoxi/phoxi.intr

#单个物体也可以用dexnet4.0的参数
python3 examples/policy.py GQCNN-4.0-PJ --depth_image data/examples/single_object/primesense/depth_0.npy --segmask data/examples/single_object/primesense/segmask_0.png --camera_intr data/calib/primesense/primesense.intr
```



##### (4) 训练数据

```
python3 tools/finetune.py data/training/example_pj/ GQCNN-4.0-PJ --config_filename cfg/finetune_example_pj.yaml --name gqcnn_example_pj
```



**训练dexnet2.0**

```
python3 tools/train.py data/training/gqcnn_data/ --config_filename cfg/train_dex-net_2.0.yaml --name d
```



#### 7.1.2 整体框架

> 下面是分开每个文件夹进行了解

##### (1) gqcnn

这里面是比较主要的代码,其他的都只是一些小工具之类的代码

###### a. analysis

​	这个文件夹中包含的是analyzer.py的文件,这里面实现的类主要是用于进行训练好的gqcnn的train_result,val_result的计算,另外可以进行曲线的绘制等工作



###### b. grasping

- image_grasp_sampler.py

  这里面有针对DepthImage的抓取采样,和本身在dexnet-1.0中的grasp_sampler并不相同.方法主要是AntipodalDepthImage的采样方法

- grasp_quality_function.py

  这里面是一系列的抓取质量评估方法,里面的sunction的方法偏多,而grasp的好像就是一个.另外还有如fcgqcnn等的上层API等的工作

- grasp.py

  这里面对抓取和sunction两个动作都进行了抽象,内部包含他们的位置,接近方向等等的内容



###### c. network

这里面主要是实现了fc_netowrk_tf.py和network_tf.py,是tensorflow构建出了网络结构的部分

- network_tf.py

  使用GQCNNTF的类进行封装



###### d. search

好像是用于搜索超参数从而提升抓取效果的,感觉这里面有很多代码没啥用的



###### e. training

这里面就一个trainer_tf.py的文件,从train()函数入手,然后到_train()函数,一步步深入



###### f. utils

一些杂七杂八的功能性代码





##### (2) tools

这里面有网络训练代码,执行推理代码,超参数搜索代码,分析gqcnn抓取效果代码,各种功能都在这里面,之后主要就是针对这里面的代码进行啃了



##### (3) examples

这里面主要就是policy.py的代码,这个用于进行抓取检测的







#### 7.1.3 图像推理

基于examples/policy.py这个文件进行效果识别,gqcnn2和4都可以应用于相同的参数,然后针对不同的抓取场景





#### 7.1.4 实际训练

##### (1) OverView

基于tools/finetune.py进行训练,还可以直接在tensorboard中打开查看对应的效果

当然使用tools/train.py也可以进行训练,这两个文件没有太大区别,只是一个有预训练模型,另外一个没有,仅此而已

(这里面split_name不清楚是什么,赋值为了image_wise)



##### (2) 核心训练

> 核心训练代码是在GQCNNTrainerTF的类中的train()函数进行实现
>
> 在tools/train.py中对训练过程进行了打包

###### a. 上层代码

- 通过get_gqcnn_model,以及gqcnn_params生成一个gqcnn网络;

- 主要是在model/network_tf.py中实现,这里面相当于是基于train.yaml文件进行网络内部参数确定,然后进行网络生成

- 通过get_gqcnn_trainer,生成一个训练数据trainer的类,从而使用trainer.train()进行训练

- 主要是在training/trainer_tf.py实现,基于网络,数据集路径,输出路径,训练配置参数,split_name构成;

- 最核心的训练代码是train()一路跟踪下去,到optimize_weights()的函数



###### b. 梯度求导代码

- 是在trainer.optimize_weights()中的函数进行实现,写sess的方式不是很记得了,但是整体框架是送入images.poses,label,然后进行

- 这里面送入的数据是在```_load_and_enqueue()```中进行实现,对于dexnet_tf的方式,则需要mask,否则就不需要mask





###### C. 数据读取

这里面甚至使用了multi_process进行多线程数据读取,这个多线程读取本身就直接是multiprocessing内置的一个函数了

是通过train_tf.py中的```_load_and_enqueue()```进行数据读取,从而拿到结果.这里面,本质上是要拿到train_images_tensor,train_poses_tensor,train_labels_tensor,这些都是从npy文件里面拿到,对应了tf_depth_ims,grasps,grasp_metrics三个中拿到即,本质上,就需要深度图,poses数据(应该是对应了另一个方向的输入),以及通过分析力闭合等的判断的0/1的label

在dexnet_fc的训练模式中,会需要同时导入mask的数据,不过其他的训练不需要这个参数,这个和angular_bins这个参数相关,可能是fc内部的一些东西,所以也可以不用管,因此,最终就是需要生成images,poses,labels三组数据即可

最后还进行了一个封装的操作,吧train_images,train_poses,train_labels,这几个进行打包返回,从而可以通过多线程把数据读取出去



- 更底层这个是通过aotulab_core中的TensorDataset进行读取的

```python
#首选打开数据集地址
dataset = TensorDataset.open(self.dataset_dir)
#dataset.tensor其实是一个dict,额你不的im_filed_name相当于指定到了一个对应的矩阵
train_images_tensor = dataset.tensor(self.im_field_name,file_num)
train_poses_tensor = dataset.tensor(self.pose_field_name,file_num)
train_labels_tensor = dataset.tensor(self.label_field_name,file_num)
```





##### (3) 实际代码阅读

> 整体入口是从tools/train.py中进行的

###### a. tools/train.py

> 这个是大入口,最终使用GQCNNTrainerTF.train()函数进行训练

首先是分别从models和trainning两个文件夹中的\_\_init\_\_中获取get_gqcnn_model和get_gqcnn_trainer两个函数,这两个函数分别返回GQCNNTF 和GQCNNTrainerTF两个主要的类

最后使用一个GQCNNTrainerTF.train()函数,就可以进行网路的训练了

这两个类都需要依赖于config文件进行配置;内部的文件还是非常多的,需要一点点的去看,规定了如训练集数据尺寸,batchsize,标注数据名称等等的内容

**GQCNNTF类定义需要**

主要是往里面给gqcnn_params的内容,这个是在config['gqcnn']的索引中的内容,规定了gqcnn网络的卷积层大小等的内容



**GQCNNTrainerTF类定义需要**

- GQCNNTF中拿到的网络类
- 数据集的dir
- split_name(分割的名称,默认是image_wise),这个也有可能是可以object_wise的分割?
- output_dir:在models文件夹下生成的一个新的训练记录数据
- train_config:对应的整体的训练内容

其本身的定义初始化就是记录了一堆的self.cfg作为数据保存的内容



**针对dexnet的训练命令:**

```
python3 tools/train.py data/training/gqcnn_data/ --config_filename cfg/train_dex-net_2.0.yaml --name train_dexnet2_0114
```



###### b. GQCNNTrainerTF

在训练中,本身这个类的定义非常简单,就是导入了一堆cfg的参数,然后就开始执行trian()这个函数,这个函数主要包含了setup(), gqcnn.initializa_network() 以及optimize_weights()三个函数



1. set_up()

   然后执行train()函数,这个函数首选先进行了基本setup,比如保存训练数据,设置数据分割等的内容

   比如setup_tensorflow中,这里面把对应的session进行返回,定义为了self.sess

2. initialize_network():

   initialize_network中初始化了送入的image_node和pose_node,相当于是初始化了一个placeholder

3. 进行optimize_weights()

   这个函数主要就是训练的全部内容了,也是最需要啃的地方.这里面首先是进行了一系列的setup,然后才开始进入到optimization loop中进行训练



###### C. optimize_weights()

###### c1. setup部分

这里面的setup包含了

- 设置网络层输出(即最后是否需要加入一个softmax/sigmoid等)
- 加入self.saver=tf.trian.Saver()
- 加入损失函数,learning_rate_step,以及网络内部参数
- 加入优化器
- 加入self.summary_writer作为tensorboard的训练过程记录板



**之后进入到try的大区域保证中:**

- 设置了读取数据的workers(这里面的如何开启多进程读数据还是很有意思的的)
- 初始化网络中的参数
- 针对angular_bins是否大于零,判断是否使用mask参数(只有在dexnet4.0中才有这个情况),其他的angular_bins模式为0



###### c2. 实际训练

在```for step in training_range:```这个的循环中开始

但是很快就发现这个网路训不动,这里面有非常多的问题,翻看github也看到了很多人在吐槽这个问题,比较相关的一个是:

https://github.com/BerkeleyAutomation/gqcnn/issues/114

为此,切换到2020年5月份的代码,然后测试一下,看看其中会不会有什么很大的不同











##### (4) 训练数据解析

> 由于训练数据读取中的内容太多了,而且是最核心的,因此提升一个层次进行说明
>
> 核心是从_load_and_enquene函数中进行执行的

###### a. 整体使用

- signal.signal是什么东西来的?



- 通过TensorDataset.open()打开了数据集的路径
- mutilprocessor的函数库感觉还是很牛逼的...
- 



在本身生成的dataset中,有make_split进行训练集和测试机的区分代码



使用tf.train.Saver()类进行数据的保存,每个step都会进行结果保存





最终通过self.prefetch_q.put_nowait()函数,把数据送出去,包含train_images,train_poses,train_labels的训练数据



###### b. TensorDataset

> 这个是读取数据集最核心的类,从autolab_core中进行实现



###### c. _load_and_enqueue

这个函数里面是使用了TensorDataset的参数,指定了对应的文件夹,然后读取了对应的数据,通过multiprocessor返回了数据,从而进行训练







#### 7.1.5 数据分析

> 使用了tools/analyze_gqcnn_performance.py进行,主要是基于gqcnn/gqcnn/analysis中的GQCNNAnalyzer的类进行分析的

##### (1) 入口文件





##### (2) analyze函数

> 主要的功能是实现是在GQCNNAnalyzer.analyze()函数中实现的

###### a. 函数输入

包含了model_dir,output_dir,dataset_config三个参数

model_dir要求的是这个文件夹下面包含一个config.json的文件

output_dir即analysis的大文件夹

dataaset_config:可以为None,对应的一个Yaml的Dict,之后大概率用别的进行替代



###### B. 实际执行

- 初始化model_output_dir

  model_output_dir是output_dir+model_name

  

- 运行prediction函数

  输入:model_dir,model_output_dir,dataset_config

  通过run_prediction_single_model函数

  输出对应的train_result和val_result

  

- 运行_plot函数,获取所有图片

  输入:model_dir,model_output_dir,train_result,val_result

  通过_plot函数

  输出对应的一系列绘制图片结果

整体内容工中包含了两个函数:

```
def _run_prediction_single_model
def _plot
```





###### b1. run_prediction

如果dataset_config为空,则选择model_config.model_config即路径下对应的config.json文件

然后进行了实际数据的预测,实际代码如下所示,导入了image_arr,pose_arr,metric_arr,label_arr

gqcnn本身只需要image_arr和pose_arr两个参数进行运算.得到了所有的predictions内容

![image-20220205194400054](picturepicture/image-20220205194400054.png)



只有,再使用BinaryClassificationResult的类进行了这两个内容里面的搬运工作

![image-20220205194709970](picturepicture/image-20220205194709970.png)

从而进行分析.这个类里面有一系列函数的实现,比如计算TP,TN,FP,FN



###### b2. plot

